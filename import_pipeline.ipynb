{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "import hashlib\n",
    "\n",
    "# Define constants ----------------------------------\n",
    "load_dotenv()\n",
    "mongoClient = MongoClient(os.environ.get('MONGO_URI_PV'))\n",
    "database = mongoClient.pv_data_db\n",
    "coll_ausgaben = database.ausgaben\n",
    "coll_artikel = database.artikel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Import PDF-files, extract text, save to txt-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf as fitz\n",
    "import os\n",
    "\n",
    "# Ordnerpfad\n",
    "pdf_folder = 'ausgaben'\n",
    "text_folder = 'ausgaben_txt'\n",
    "\n",
    "# Erstellen Sie den Ordner für die Textdateien, falls er nicht existiert\n",
    "if not os.path.exists(text_folder):\n",
    "    os.makedirs(text_folder)\n",
    "\n",
    "# Durchlaufen Sie alle PDF-Dateien im Ordner\n",
    "for filename in os.listdir(pdf_folder):\n",
    "    if filename.endswith('.pdf'):\n",
    "        pdf_path = os.path.join(pdf_folder, filename)\n",
    "        text_path = os.path.join(text_folder, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "        \n",
    "        # PDF-Datei öffnen\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        \n",
    "        # Text aus der PDF-Datei extrahieren\n",
    "        text = \"\"\n",
    "        for page_num in range(pdf_document.page_count):\n",
    "            page = pdf_document.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "        \n",
    "        # Text in eine Textdatei schreiben\n",
    "        with open(text_path, 'w', encoding='utf-8') as text_file:\n",
    "            text_file.write(text)\n",
    "\n",
    "print(\"Text aus allen PDF-Dateien wurde extrahiert und gespeichert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30. Import Ausgaben im TXT Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_folder = \"ausgaben_txt\"\n",
    "\n",
    "for filename in os.listdir(txt_folder):\n",
    "    # Open file and read content\n",
    "    with open(f\"{txt_folder}/{filename}\", \"r\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Create doknr\n",
    "    ausgabe = filename[:-4].split(\" \")[2][1:3]\n",
    "    jahrgang = filename[:-4].split(\" \")[3]\n",
    "    doknr = f\"{jahrgang}.{ausgabe}\"\n",
    "\n",
    "    # Check if hash already exists in database\n",
    "    if coll_ausgaben.find_one({\"doknr\": doknr}):\n",
    "        print(f\"File {filename} already exists in database\")\n",
    "    else:\n",
    "        # Insert new document into database\n",
    "        coll_ausgaben.insert_one({\n",
    "            \"doknr\": doknr,\n",
    "            \"jahrgang\": int(jahrgang),\n",
    "            \"ausgabe\": int(ausgabe),\n",
    "            \"text\": content,\n",
    "            })\n",
    "        print(f\"File {filename} added to database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40. In Chunks zerlegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_to_dataframe(text:str, chunk_size:int, overlap:int=0) -> list:\n",
    "    \"\"\"\n",
    "    Splits a text into chunks and stores them in a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        text: The input text string.\n",
    "        chunk_size: The desired size of each chunk (number of characters).\n",
    "        overlap: The number of overlapping characters between chunks.  Defaults to 0 (no overlap).\n",
    "\n",
    "    Returns:\n",
    "        A Pandas DataFrame where each row represents a chunk of text.  Returns an empty DataFrame if the input text is None or empty.\n",
    "        Returns None if chunk_size is invalid (<= 0) or overlap is negative or greater than or equal to chunk_size.\n",
    "    \"\"\"\n",
    "\n",
    "    if not text:  # Handle None or empty input\n",
    "        return None\n",
    "\n",
    "    if chunk_size <= 0:\n",
    "      return None\n",
    "\n",
    "    if overlap < 0 or overlap >= chunk_size:\n",
    "      return None\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))  # Ensure we don't go past the end of the text\n",
    "        chunk = {'start': start, 'end': end, 'text': text[start:end]}\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap # Move start for the next chunk, accounting for overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50. Chunks in Collection ARTIKEL einfügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = coll_ausgaben.find()\n",
    "for ausgabe in cursor:\n",
    "    chunks = chunk_text_to_dataframe(ausgabe['text'], 5000, 500)\n",
    "    for chunk in chunks:\n",
    "        coll_artikel.insert_one({\n",
    "            \"doknr\": ausgabe['doknr'],\n",
    "            \"start\": chunk['start'],\n",
    "            \"end\": chunk['end'],\n",
    "            \"text\": chunk['text'],\n",
    "            \"embeddings\": [],\n",
    "            # \"hash\": hashlib.md5(chunk['text'].encode()).hexdigest()\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = coll_artikel.find()\n",
    "for artikel in cursor:\n",
    "    print(f\"{artikel['doknr']} [{artikel['start']}-{artikel['end']}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 60. Embeddings in Collection ARTIKEL generieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mweber/dev/pvbuddy/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "model_name = \"bert-base-german-cased\" # 768 dimensions\n",
    "# model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Embeddings -------------------------------------------------            \n",
    "def create_embeddings(text: str) -> list:\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    return model_output.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
    "\n",
    "def generate_embeddings(input_field: str, output_field: str, \n",
    "                        max_iterations: int = 0) -> None:\n",
    "    if max_iterations != 0:\n",
    "        cursor = coll_artikel.find({output_field: []}).limit(max_iterations)\n",
    "    else:\n",
    "        cursor = coll_artikel.find({output_field: []})\n",
    "    cursor_list = list(cursor)\n",
    "    for record in cursor_list:\n",
    "        article_text = record[input_field]\n",
    "        if article_text == \"\":\n",
    "            article_text = \"Fehler: Kein Text vorhanden.\"\n",
    "        else:\n",
    "            embeddings = create_embeddings(text=article_text)\n",
    "            coll_artikel.update_one({\"_id\": record['_id']}, {\"$set\": {output_field: embeddings}})\n",
    "    print(f\"\\nGenerated embeddings for {len(cursor_list)} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated embeddings for 0 records.\n"
     ]
    }
   ],
   "source": [
    "generate_embeddings(\"text\", \"embeddings\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
