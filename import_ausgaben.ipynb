{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "import hashlib\n",
    "\n",
    "# Define constants ----------------------------------\n",
    "load_dotenv()\n",
    "mongoClient = MongoClient(os.environ.get('MONGO_URI_PV'))\n",
    "database = mongoClient.pv_data_db\n",
    "coll_ausgaben = database.ausgaben\n",
    "coll_artikel = database.artikel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Import PDF-files, extract text, save to txt-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text aus allen PDF-Dateien wurde extrahiert und gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import pymupdf as fitz\n",
    "import os\n",
    "\n",
    "# Ordnerpfad\n",
    "pdf_folder = 'ausgaben'\n",
    "text_folder = 'ausgaben_txt'\n",
    "\n",
    "# Erstellen Sie den Ordner für die Textdateien, falls er nicht existiert\n",
    "if not os.path.exists(text_folder):\n",
    "    os.makedirs(text_folder)\n",
    "\n",
    "# Durchlaufen Sie alle PDF-Dateien im Ordner\n",
    "for filename in os.listdir(pdf_folder):\n",
    "    if filename.endswith('.pdf'):\n",
    "        pdf_path = os.path.join(pdf_folder, filename)\n",
    "        text_path = os.path.join(text_folder, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "        \n",
    "        # PDF-Datei öffnen\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        \n",
    "        # Text aus der PDF-Datei extrahieren\n",
    "        text = \"\"\n",
    "        for page_num in range(pdf_document.page_count):\n",
    "            page = pdf_document.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "        \n",
    "        # Text in eine Textdatei schreiben\n",
    "        with open(text_path, 'w', encoding='utf-8') as text_file:\n",
    "            text_file.write(text)\n",
    "\n",
    "print(\"Text aus allen PDF-Dateien wurde extrahiert und gespeichert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25. Test pypdf & visitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seiten: 31\n",
      "--------------------------------------------------\n",
      " \n",
      "ZEITUNGEN, ZEITSCHRIFTEN, PAID CONTENT \n",
      "INFORMATIONEN FÜR DEN VERLEGERISCHEN ERFOLG IM LESERMARKT  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "STOP!  Bitte stellen Sie sicher, dass Sie das sind. Wenn nicht, dann lesen Sie in einer unlizenzierten Ausgabe. Das gefährdet das Erscheinen von pv digest! Für \n",
      "mehrere Leser innerhalb eines Unternehmens haben wir attraktive Konditionen. Sprechen Sie uns dazu gerne an: abo@pv-digest.de / Tel.: 040 / 308 901 09. \n",
      "AUSGABE 11/2024 ׀SEITE 4 \n",
      " \n",
      "Zwei Jahre L'Equipe zum Schnäppchenpreis – und dann? \n",
      "Mitte Oktober veranstaltete der deutsche (Digital -)Abomanagement-Dienstleister Plenigo \n",
      "seinen 'Community Day'. Eine Sprecherin auf der sehr gut besuchten Veranstaltung war die \n",
      "Mitgründerin des französischen Paywallmanagement- Dienstleisters Poool [pvd: hier bahnt \n",
      "sich vielleicht eine sehr spannende Allianz an. Plenigo würde in Frankreich einen großen Markt \n",
      "vorfinden – Poools Ansatz bei der Paywallsteuerung (vgl. auch  \n",
      "S. 24), wäre für viele Paid Content-Anbieter im D/A/CH-Raum ein \n",
      "großer Schritt nach vorne]. \n",
      "Marion Wyss präsentierte den Case eines 'Flash Sales'  Ihres \n",
      "Kunden L'Equipe. Wie das ganze Land hatte die große französi-\n",
      "sche Sport -Tageszeitung mit über 200.000 Digitalabonnenten \n",
      "(vgl. pvd zuletzt pvd #6/2024) den olympischen Spielen in die-\n",
      "sem Sommer lange entgegengefiebert. Schon zwei Jahre vorher, \n",
      "im Sommer 2022, nutzte L'Equipe das Thema für einen 'Flash \n",
      "Sale'. Es warb \" tausende\" Digitalabonnenten für den Mini-\n",
      "preis von 2,24€ pro Monat, der in der Visualisierung mit der Jah-\n",
      "reszahl 2024 spielt. \n",
      "Rund zwei Jahre nach der Werbung dieser Abonnenten und 1 Monat vor Beginn der olym-\n",
      "pischen Spiele in Paris, am 14. Ju ni 2024, war der große Tag für diese Aktion. Der Preis der \n",
      "Billigabos wurde auf den Regelpreis angehoben, von 2,24€ pro Monat auf 11,99€ pro Mo-\n",
      "nat, ein Prei ssprung von +400%! Wie viele dieser Abonnenten würden bei Stange bleiben? \n",
      "Rund vier Monate nach dem Stichtag präsentierte Wyss nun eine Zwei-Monats-Aktivquote. \n",
      "Ohne besondere Maßnahmen bezahlten im zweiten Monat zum Regelpreis noch 63%  der \n",
      "Schnäppchenabonnenten für das nun reguläre Digitalabo. \n",
      "Aber es kam noch besser. Wyss stellte auf der Plenigo-Veranstaltung eine von Poool gesteu-\n",
      "erte Maßnahmenkette zur Kundenbindung vor. Beginnend 90 Tage vor der Preisanhebung \n",
      "und bis in die Monate nach dem Preisschritt steuerte Poool eine Serie von kommunikativen \n",
      "Anstößen, die zum Ziel hatten, loyalitätsfördernde Nutzungsgewohnheiten bei den Abonnen-\n",
      "ten zu befördern. Mit Bannern auf der Website wurde für die Installation der App geworben. \n",
      "Appetizer-Werbung versprach den Abonnenten begeisternde Videoinhalte [L'Equipes Digital-\n",
      "angebot ist multimedial sehr umfangreich]. Extra für die Billigabonnenten wurde ein e Ban-\n",
      "nerkampagne orchestriert, die diese via \"1-Klick-Registrierung\" für den Morgennewsletter ge-\n",
      "winnen sollte. Schließlich spielten die Marketer auch mit der Verlustangst.  \"Schon bald wirst \n",
      "Du keinen Zugang zu diesem Artikel mehr haben\" drohte ein Banner, das noch in der Billig-\n",
      "phase bei Kündigern auf besonders attraktiven Inhalten platziert wurde und das zu einer \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "2 Jahre für 2,24€, \n",
      "dann 11,99€ bei \n",
      "L'Equipe \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "2 Monate nach der \n",
      "Preiserhöhung um \n",
      "+400% noch mind. \n",
      "63% der Abos aktiv \n",
      " \n",
      " \n",
      "sogar 84% der Abon-\n",
      "nenten, die mind. einer \n",
      "dieser Maßnahmen be-\n",
      "gegnet sind: \n",
      "• App-Install-Banner \n",
      "• Video-Appetizern \n",
      "• Newsletter-Registrie-\n",
      "rungs-Promotion \n",
      "• Kündigerbanner \n",
      "[mit Sonderpreis!] \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "pvd meint: Der Boston Globe gehört dem Investment-Milliardär John W Henry. Der hat \n",
      "die Zeitung gewiss nicht erworben, um damit Traumrenditen zu erzielen. Aber ein nachhal-\n",
      "tiges Wirtschaften verlangt er schon. Wenn Henry seit nunmehr 5 Jahren die Abonnenten-\n",
      "Akquise zum Schnäpp chenpreis duldet, dann ist das ein ziemlich harter Indikator dafür, \n",
      "dass die Rechnung tatsächlich aufgeht. Noch härter vielleicht als das seitdem und noch 6 \n",
      "Jahre länger ungebrochene Wachstum des Digitalabobestandes. \n",
      "pv digest - persönliches Exemplar für  Herrn Martin Weber, DVV Media GmbH\n",
      "--------------------------------------------------\n",
      "Parts: 12\n",
      "['', '\\n', '', '', '', '', '', '', '', '', '\\n', '']\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pypdf\n",
    "\n",
    "parts = []\n",
    "\n",
    "def visitor_body(text, cm, tm, font_dict, font_size):\n",
    "    y = cm[5]              # y coordinate\n",
    "    header_max_y = 900     # header\n",
    "    footer_min_y = 85     # footer\n",
    "    if footer_min_y < y < header_max_y:\n",
    "        parts.append(text)\n",
    "\n",
    "file_name = 'ausgaben/pvd Ausgabe #11 2024.pdf'\n",
    "pdf_doc = pypdf.PdfReader(file_name)\n",
    "\n",
    "print(f\"Seiten: {len(pdf_doc.pages)}\")\n",
    "\n",
    "text = pdf_doc.pages[3].extract_text()\n",
    "        \n",
    "# for page in pdf_doc.pages:\n",
    "pdf_doc.pages[3].extract_text(visitor_text=visitor_body)\n",
    "\n",
    "text_visitor = \"\".join(parts)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(text)\n",
    "print(\"-\" * 50)\n",
    "print(f\"Parts: {len(parts)}\")\n",
    "print(parts)\n",
    "print(\"-\" * 50)\n",
    "print(text_visitor)\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30. Import Ausgaben im TXT Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_folder = \"ausgaben_txt\"\n",
    "\n",
    "for filename in os.listdir(text_folder):\n",
    "    # Open file and read content\n",
    "    with open(f\"{text_folder}/{filename}\", \"r\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Create doknr\n",
    "    ausgabe = filename[:-4].split(\" \")[2][1:3]\n",
    "    jahrgang = filename[:-4].split(\" \")[3]\n",
    "    doknr = f\"{jahrgang}.{ausgabe}\"\n",
    "\n",
    "    # Check if hash already exists in database\n",
    "    if coll_ausgaben.find_one({\"doknr\": doknr}):\n",
    "        print(f\"File {filename} already exists in database\")\n",
    "    else:\n",
    "        # Insert new document into database\n",
    "        coll_ausgaben.insert_one({\n",
    "            \"doknr\": doknr,\n",
    "            \"jahrgang\": int(jahrgang),\n",
    "            \"ausgabe\": int(ausgabe),\n",
    "            \"text\": content,\n",
    "            })\n",
    "        print(f\"File {filename} added to database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40. In Chunks zerlegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_to_dataframe(text:str, chunk_size:int, overlap:int=0) -> list:\n",
    "    \"\"\"\n",
    "    Splits a text into chunks and stores them in a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        text: The input text string.\n",
    "        chunk_size: The desired size of each chunk (number of characters).\n",
    "        overlap: The number of overlapping characters between chunks.  Defaults to 0 (no overlap).\n",
    "\n",
    "    Returns:\n",
    "        A Pandas DataFrame where each row represents a chunk of text.  Returns an empty DataFrame if the input text is None or empty.\n",
    "        Returns None if chunk_size is invalid (<= 0) or overlap is negative or greater than or equal to chunk_size.\n",
    "    \"\"\"\n",
    "\n",
    "    if not text:  # Handle None or empty input\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if chunk_size <= 0:\n",
    "      return None\n",
    "\n",
    "    if overlap < 0 or overlap >= chunk_size:\n",
    "      return None\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))  # Ensure we don't go past the end of the text\n",
    "        chunk = {'start': start, 'end': end, 'text': text[start:end]}\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap # Move start for the next chunk, accounting for overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50. Chunks in Collection ARTIKEL einfügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = coll_ausgaben.find()\n",
    "for ausgabe in cursor:\n",
    "    if coll_artikel.find_one({\"doknr\": ausgabe[\"doknr\"]}):\n",
    "        print(f\"Document {ausgabe[\"doknr\"]} already exists in database\")\n",
    "    else:\n",
    "        chunks = chunk_text_to_dataframe(ausgabe['text'], 1000, 200)\n",
    "        for chunk in chunks:\n",
    "            coll_artikel.insert_one({\n",
    "                \"doknr\": ausgabe['doknr'],\n",
    "                \"start\": chunk['start'],\n",
    "                \"end\": chunk['end'],\n",
    "                \"text\": chunk['text'],\n",
    "                \"embeddings\": [],\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = coll_artikel.find()\n",
    "for artikel in cursor:\n",
    "    print(f\"{artikel['doknr']} [{artikel['start']}-{artikel['end']}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 60. Embeddings in Collection ARTIKEL generieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "model_name = \"bert-base-german-cased\" # 768 dimensions\n",
    "# model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Embeddings -------------------------------------------------            \n",
    "def create_embeddings(text: str) -> list:\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    return model_output.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
    "\n",
    "def generate_embeddings(input_field: str, output_field: str, \n",
    "                        max_iterations: int = 10) -> None:\n",
    "    cursor = coll_artikel.find({output_field: []}) #.limit(max_iterations)\n",
    "    cursor_list = list(cursor)\n",
    "    for record in cursor_list:\n",
    "        article_text = record[input_field]\n",
    "        if article_text == \"\":\n",
    "            article_text = \"Fehler: Kein Text vorhanden.\"\n",
    "        else:\n",
    "            embeddings = create_embeddings(text=article_text)\n",
    "            coll_artikel.update_one({\"_id\": record['_id']}, {\"$set\": {output_field: embeddings}})\n",
    "    print(f\"\\nGenerated embeddings for {max_iterations} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated embeddings for 10 records.\n"
     ]
    }
   ],
   "source": [
    "generate_embeddings(\"text\", \"embeddings\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
